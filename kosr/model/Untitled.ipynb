{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from attention import *\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, max_length=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, dim_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        exp_term = torch.exp(torch.arange(0, dim_model, 2).float() * -(math.log(10000.0) / dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * exp_term)\n",
    "        pe[:, 1::2] = torch.cos(position * exp_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.pe[:, :input.size(1)]\n",
    "\n",
    "class FeedForwardNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = nn.SELU()\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import MultiHeadAttention, RelPositionMultiHeadAttention\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, n_head, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = RelPositionMultiHeadAttention(n_head, hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):  # pylint: disable=arguments-differ\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x, mask\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, n_head, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_size, filter_size, n_head, dropout_rate)\n",
    "                    for _ in range(n_layers)])\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        encoder_output = inputs\n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output, mask = enc_layer(encoder_output, mask)\n",
    "        return self.last_norm(encoder_output), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import MultiHeadAttention, RelPositionMultiHeadAttention\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, n_head, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(n_head, hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(n_head, hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, self_mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output, i_mask,\n",
    "                                       cache)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, n_head, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_size, filter_size, n_head, dropout_rate)\n",
    "                    for _ in range(n_layers)])\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, targets, enc_output, tgt_mask, enc_mask, cache):\n",
    "        decoder_output = targets\n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            layer_cache = None\n",
    "            if cache is not None:\n",
    "                if i not in cache:\n",
    "                    cache[i] = {}\n",
    "                layer_cache = cache[i]\n",
    "            decoder_output = dec_layer(decoder_output, enc_output, tgt_mask, enc_mask, layer_cache)\n",
    "        return self.last_norm(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from feature_extractor import *\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        feat_extractor='vgg', \n",
    "        enc_n_layers=16, \n",
    "        dec_n_layers=1, \n",
    "        hidden_size=512, \n",
    "        filter_size=2048,\n",
    "        n_head=8,\n",
    "        dropout_rate=0.1, \n",
    "        pad_id=0, \n",
    "        sos_id=1, \n",
    "        eos_id=2,\n",
    "        init_type=\"xavier_uniform\"\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.init_type = init_type\n",
    "        \n",
    "        if feat_extractor=='vgg':\n",
    "            self.conv = VGGExtractor()\n",
    "        elif feat_extractor=='w2v':\n",
    "            self.conv = W2VExtractor()\n",
    "            \n",
    "        self.encoder = Encoder(hidden_size, filter_size, n_head,\n",
    "                               dropout_rate, enc_n_layers)\n",
    "        \n",
    "        self.decoder = Decoder(hidden_size, filter_size,n_head,\n",
    "                               dropout_rate, dec_n_layers)\n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "    def forward(self, padded_input, input_lengths, padded_target):\n",
    "        if self.feat_extractor == 'vgg' or self.feat_extractor == 'w2v':\n",
    "            padded_input = self.conv(padded_input)\n",
    "\n",
    "        # Reshaping features\n",
    "        sizes = padded_input.size() # B x H_1 (channel?) x H_2 x T\n",
    "        padded_input = padded_input.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        padded_input = padded_input.transpose(1, 2).contiguous()  # BxTxH\n",
    "\n",
    "        encoder_padded_outputs, _ = self.encoder(padded_input, input_lengths)\n",
    "        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs, input_lengths)\n",
    "        hyp_best_scores, hyp_best_ids = torch.topk(pred, 1, dim=2)\n",
    "\n",
    "        hyp_seq = hyp_best_ids.squeeze(2)\n",
    "        gold_seq = gold\n",
    "\n",
    "        return pred, gold, hyp_seq, gold_seq\n",
    "\n",
    "    def initialize(self):\n",
    "        # weight init\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                if self.init_type == \"xavier_uniform\":\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif self.init_type == \"xavier_normal\":\n",
    "                    nn.init.xavier_normal_(p.data)\n",
    "                elif self.init_type == \"kaiming_uniform\":\n",
    "                    nn.init.kaiming_uniform_(p.data, nonlinearity=\"relu\")\n",
    "                elif self.init_type == \"kaiming_normal\":\n",
    "                    nn.init.kaiming_normal_(p.data, nonlinearity=\"relu\")\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown initialization: \" + self.init_type)\n",
    "        # bias init\n",
    "        for p in self.parameters():\n",
    "            if p.dim() == 1:\n",
    "                p.data.zero_()\n",
    "\n",
    "        # reset some modules with default init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Embedding, nn.LayerNorm)):\n",
    "                m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59179968"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in Transformer(512).parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dncoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74de42663218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkosr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/storage/git/Korean-Online-Speech-Recognition/kosr/model/transformer/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkosr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkosr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkosr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Dncoder'"
     ]
    }
   ],
   "source": [
    "from kosr.model.transformer.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kosr.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'Transformer' object has no attribute 'feat_extractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be78bd142c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/storage/git/Korean-Online-Speech-Recognition/kosr/model/transformer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, padded_input, input_lengths, padded_target)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vgg'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extractor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w2v'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'Transformer' object has no attribute 'feat_extractor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model(torch.randn(16,1,52,80), torch.randint(52,(16,52)), torch.randint(450,(16,250)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 52])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(52,(16,52)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[:100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(256).masked_fill(mask.eq(0), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kosr.model import mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.subsequent_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randint(16,(16,1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(1,25,(1,280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 280])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size, device=\"cpu\", dtype=torch.bool):\n",
    "    ret = torch.ones(size, size, device=device, dtype=dtype)\n",
    "    return torch.tril(ret, out=ret)\n",
    "\n",
    "\n",
    "def target_mask(ys_in_pad, ignore_id):\n",
    "    ys_mask = ys_in_pad != ignore_id\n",
    "    print(ys_mask)\n",
    "    m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n",
    "    print(m)\n",
    "    return (ys_mask.unsqueeze(-2) & m).eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True, False]])\n",
      "tensor([[[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ...,  True, False, False],\n",
      "         [ True,  True,  True,  ...,  True,  True, False],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "r = target_mask(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False, False, False],\n",
       "        [ True,  True, False,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ...,  True, False, False],\n",
       "        [ True,  True,  True,  ...,  True,  True, False],\n",
       "        [ True,  True,  True,  ...,  True,  True, False]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 280])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 280])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(30,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def get_decoder_self_attn_mask(seq_k: Tensor, seq_q: Tensor, pad_id):\n",
    "    \"\"\" For masking the decoder self attention \"\"\"\n",
    "    def _get_attn_key_pad_mask(seq_k, seq_q, pad_id):\n",
    "        \"\"\" For masking out the padding part of key sequence. \"\"\"\n",
    "        len_q = seq_q.size(1)\n",
    "        padding_mask = seq_k.eq(pad_id)\n",
    "        padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "        return padding_mask\n",
    "\n",
    "    def _get_subsequent_mask(inputs: Tensor) -> Tensor:\n",
    "        \"\"\" Makes subsequent masking \"\"\"\n",
    "        batch_size, seq_length = inputs.size()\n",
    "        subsequent_mask = torch.triu(torch.ones((seq_length, seq_length), device=inputs.device, dtype=torch.uint8), diagonal=1)\n",
    "        subsequent_mask = subsequent_mask.unsqueeze(0).expand(batch_size, -1, -1)  # BxTxT\n",
    "\n",
    "        return subsequent_mask.bool()\n",
    "\n",
    "    return _get_attn_key_pad_mask(seq_k, seq_q, pad_id) | _get_subsequent_mask(seq_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(30,(1,280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_decoder_self_attn_mask(a,a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False, False,  True,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False,  True,  True],\n",
       "         [False, False, False,  ..., False, False,  True],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = emb(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  ...,  True,  True,  True],\n",
       "         [False, False,  True,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False,  True,  True],\n",
       "         [False, False, False,  ..., False, False,  True],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 280])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(1,280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0901,  0.3619,  ...,  1.5084, -0.4110,  0.3909],\n",
       "        [ 0.0000,  0.0000,  0.3619,  ...,  1.5084, -0.4110,  0.3909],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  1.5084, -0.4110,  0.3909],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.4110,  0.3909],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3909],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3909]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.masked_fill(r[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (280) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-79d956c544a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (280) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "b.masked_fill(mask, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 280, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
